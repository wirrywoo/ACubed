\section{Stepfile Difficulty Model}
\label{sec:stepfile_difficulty}

Stepfile difficulty has been a recurring topic of debate within the FFR community. The task to define how difficult it is for a player to execute a specific sequence of keystrokes requires manual effort to study shared patterns across stepfiles, making it highly susceptible to player subjectivity. Despite efforts to address this issue through the use of machine learning and artificial intelligence, no definitive solutions have yet been identified. This is largely due to the fact that individual players have varying interpretations of what constitutes a difficult stepfile, informed by their own unique playing objectives and physical abilities. As a result, community resolutions on stepfile difficulty often give rise to biased judgments and disagreements. In order to effectively address these concerns, we must first standardize the definition of stepfile difficulty, then differentiate between the associated objective characteristics from its inherent subjectivity.

\subsection{Various Definitions of Stepfile Difficulty}

The aforementioned playing objectives can be categorized into two groups: constant weighted and variable weighted. This section focuses on providing numerous examples of both types of playing objectives, discussing their integration into our analysis, and establishing a default playing objective to be utilized in this report. It is imperative to acknowledge that, at present, ACubed currently supports only stepfile difficulty measurements under discretized constant weighted playing objectives. Future developments of ACubed are anticipated to broaden its support to include other playing objectives.

\subsubsection{Constant-Weighted Playing Objectives}

\begin{figure}[H]
\centering
\begin{tikzpicture}

    \begin{scope}
        \shade[top color=perfect-color, bottom color=perfect-color] (-1,-1) rectangle (4,0);
\shade[top color=perfect-color, bottom color=perfect-color] (-1,0) rectangle (4,1);
\node[inner sep=0pt] (left) at (0,0)
    {\includegraphics[width=1cm]{figures/receptor/left.png}};
\node[inner sep=0pt] (down) at (1,0)
    {\includegraphics[width=1cm]{figures/receptor/down.png}};
\node[inner sep=0pt] (up) at (2,0)
    {\includegraphics[width=1cm]{figures/receptor/up.png}};
\node[inner sep=0pt] (right) at (3,0)
    {\includegraphics[width=1cm]{figures/receptor/right.png}};
\node[inner sep=0pt] at (-1,-1)[label=left:{\footnotesize\texttt{-117ms}}]{};
\draw[opacity=0.5,dashed] (-1, -1) -- (4, -1); 
\node[inner sep=0pt] at (-1,0) [label=left:{\footnotesize \texttt{0ms}}]{};
\draw[opacity=0.5,dashed] (-1, 0) -- (4, 0); 
\node[inner sep=0pt] at (-1,1) [label=left:{\footnotesize \texttt{118ms}}]{};
\draw[opacity=0.5,dashed] (-1, 1) -- (4, 1);
\node[inner sep=0pt] at (1.5, -1.5) [label=center:{\footnotesize \textit{(a)} Full-Combo (FC)}]{};
    \end{scope}

    \begin{scope}[xshift=8cm]
\shade[top color=perfect-color, bottom color=perfect-color] (-1,-1) rectangle (4,0);
\shade[top color=perfect-color, bottom color=perfect-color] (-1,0) rectangle (4,0.083/0.118);
\node[inner sep=0pt] (left) at (0,0)
    {\includegraphics[width=1cm]{figures/receptor/left.png}};
\node[inner sep=0pt] (down) at (1,0)
    {\includegraphics[width=1cm]{figures/receptor/down.png}};
\node[inner sep=0pt] (up) at (2,0)
    {\includegraphics[width=1cm]{figures/receptor/up.png}};
\node[inner sep=0pt] (right) at (3,0)
    {\includegraphics[width=1cm]{figures/receptor/right.png}};
\node[inner sep=0pt] at (-1,-1)[label=left:{\footnotesize\texttt{-117ms}}]{};
\draw[opacity=0.5,dashed] (-1, -1) -- (4, -1); 
\node[inner sep=0pt] at (-1,0) [label=left:{\footnotesize \texttt{0ms}}]{};
\draw[opacity=0.5,dashed] (-1, 0) -- (4, 0); 
\node[inner sep=0pt] at (-1,1) [label=left:{\footnotesize \texttt{118ms}}]{};
\draw[opacity=0.5,dashed] (-1, 1) -- (4, 1);
\node[inner sep=0pt] at (1.5, -1.5) [label=center:{\footnotesize \textit{(b)} Clean Full-Combo (FC)}]{};
    \end{scope}

    \begin{scope}[yshift=-3.5cm]
        \shade[top color=perfect-color, bottom color=perfect-color, opacity=0.5] (-1,-1) rectangle (4,-0.050/0.117);
\shade[top color=perfect-color, bottom color=perfect-color] (-1,-0.050/0.117) rectangle (4,0);
\shade[top color=perfect-color, bottom color=perfect-color] (-1,0) rectangle (4,0.050/0.118);
\shade[top color=perfect-color, bottom color=perfect-color, opacity=0.5] (-1,0.050/0.118) rectangle (4,0.083/0.118);
\shade[top color=perfect-color, bottom color=perfect-color, opacity=0.1] (-1,0.083/0.118) rectangle (4,1);
\node[inner sep=0pt] (left) at (0,0)
    {\includegraphics[width=1cm]{figures/receptor/left.png}};
\node[inner sep=0pt] (down) at (1,0)
    {\includegraphics[width=1cm]{figures/receptor/down.png}};
\node[inner sep=0pt] (up) at (2,0)
    {\includegraphics[width=1cm]{figures/receptor/up.png}};
\node[inner sep=0pt] (right) at (3,0)
    {\includegraphics[width=1cm]{figures/receptor/right.png}};
\node[inner sep=0pt] at (-1,-1)[label=left:{\footnotesize\texttt{-117ms}}]{};
\draw[opacity=0.5,dashed] (-1, -1) -- (4, -1); 
\node[inner sep=0pt] at (-1,0) [label=left:{\footnotesize \texttt{0ms}}]{};
\draw[opacity=0.5,dashed] (-1, 0) -- (4, 0); 
\node[inner sep=0pt] at (-1,1) [label=left:{\footnotesize \texttt{118ms}}]{};
\draw[opacity=0.5,dashed] (-1, 1) -- (4, 1);
\node[inner sep=0pt] at (1.5, -1.5) [label=center:{\footnotesize \textit{(c)} Maximize Score under FFR Scoring (Default)}]{};
    \end{scope}

    \begin{scope}[xshift=8cm, yshift=-3.5cm]
\shade[top color=perfect-color, bottom color=white] (-1,-1) rectangle (4,0);
\shade[top color=white, bottom color=perfect-color] (-1,0) rectangle (4,1);
\node[inner sep=0pt] (left) at (0,0)
    {\includegraphics[width=1cm]{figures/receptor/left.png}};
\node[inner sep=0pt] (down) at (1,0)
    {\includegraphics[width=1cm]{figures/receptor/down.png}};
\node[inner sep=0pt] (up) at (2,0)
    {\includegraphics[width=1cm]{figures/receptor/up.png}};
\node[inner sep=0pt] (right) at (3,0)
    {\includegraphics[width=1cm]{figures/receptor/right.png}};
\node[inner sep=0pt] at (-1,-1)[label=left:{\footnotesize\texttt{-117ms}}]{};
\draw[opacity=0.5,dashed] (-1, -1) -- (4, -1); 
\node[inner sep=0pt] at (-1,0) [label=left:{\footnotesize\texttt{0ms}}]{};
\draw[opacity=0.5,dashed] (-1, 0) -- (4, 0); 
\node[inner sep=0pt] at (-1,1) [label=left:{\footnotesize\texttt{118ms}}]{};
\draw[opacity=0.5,dashed] (-1, 1) -- (4, 1);
\node[inner sep=0pt] at (1.5, -1.5) [label=center:{\footnotesize \textit{(d)} Maximize Score under MS-based Timings}]{};
    \end{scope}

    \begin{scope}[yshift=-7cm]
\shade[top color=perfect-color, bottom color=perfect-color] (-1,-0.050/0.117) rectangle (4,0.050/0.118);
\node[inner sep=0pt] (left) at (0,0)
    {\includegraphics[width=1cm]{figures/receptor/left.png}};
\node[inner sep=0pt] (down) at (1,0)
    {\includegraphics[width=1cm]{figures/receptor/down.png}};
\node[inner sep=0pt] (up) at (2,0)
    {\includegraphics[width=1cm]{figures/receptor/up.png}};
\node[inner sep=0pt] (right) at (3,0)
    {\includegraphics[width=1cm]{figures/receptor/right.png}};
\node[inner sep=0pt] at (-1,-1)[label=left:{\footnotesize\texttt{-117ms}}]{};
\draw[opacity=0.5,dashed] (-1, -1) -- (4, -1); 
\node[inner sep=0pt] at (-1,0) [label=left:{\footnotesize \texttt{0ms}}]{};
\draw[opacity=0.5,dashed] (-1, 0) -- (4, 0); 
\node[inner sep=0pt] at (-1,1) [label=left:{\footnotesize \texttt{118ms}}]{};
\draw[opacity=0.5,dashed] (-1, 1) -- (4, 1);
\node[inner sep=0pt] at (1.5, -1.5) [label=center:{\footnotesize \textit{(e)} All Perfects (AAA)}]{};
    \end{scope}

    \begin{scope}[xshift=8cm, yshift=-7cm]
\shade[top color=perfect-color, bottom color=perfect-color] (-1,-0.017/0.117) rectangle (4,0.017/0.118);
\node[inner sep=0pt] (left) at (0,0)
    {\includegraphics[width=1cm]{figures/receptor/left.png}};
\node[inner sep=0pt] (down) at (1,0)
    {\includegraphics[width=1cm]{figures/receptor/down.png}};
\node[inner sep=0pt] (up) at (2,0)
    {\includegraphics[width=1cm]{figures/receptor/up.png}};
\node[inner sep=0pt] (right) at (3,0)
    {\includegraphics[width=1cm]{figures/receptor/right.png}};
\node[inner sep=0pt] at (-1,-1)[label=left:{\footnotesize\texttt{-117ms}}]{};
\draw[opacity=0.5,dashed] (-1, -1) -- (4, -1); 
\node[inner sep=0pt] at (-1,0) [label=left:{\footnotesize \texttt{0ms}}]{};
\draw[opacity=0.5,dashed] (-1, 0) -- (4, 0); 
\node[inner sep=0pt] at (-1,1) [label=left:{\footnotesize \texttt{118ms}}]{};
\draw[opacity=0.5,dashed] (-1, 1) -- (4, 1);
\node[inner sep=0pt] at (1.5, -1.5) [label=center:{\footnotesize \textit{(f)} All Marvelous (AAAA)}]{};
    \end{scope}

\end{tikzpicture}

\caption{Constant-Weighted Playing Objectives to define Stepfile Difficulty.} \label{fig:M1}
\end{figure}


\subsubsection{Variable-Weighted Playing Objectives}


\subsection{Machine Learning Problem Formulation}

We will begin this section by introducing some definitions and proposing a theorem:

\begin{itemize}
    \item \textit{Objective stepfile features} refer to characteristics of a stepfile that contain rigid requirements independent of the player's skill level. These may include precise timing requirements for all arrows and the duration of the stepfile. Note that meta stepfile features are not considered objective, as the established definition to differentiate patterns from one another cannot be codified deterministically. 
    \item \textit{Subjective stepfile features} are defined as elements that are primarily centered on the execution of the stepfile and are subject to the player's individual skill level. These features may include factors related to a player's ability to execute the steps accurately, as well as their access to high-quality equipment. For instance, a player with greater stamina is likely to excel at longer stepfiles and may tend to underestimate its level of challenge as a result. 
\end{itemize}

\begin{theorem}
Given a stepfile, the true difficulty rating can be estimated with a machine learning model using the objective stepfile features and the upper echelons of player telemetry.
\end{theorem}

\begin{proof}
Let Players $A_1, A_2, \cdots, A_n$ represent $n$ players to provide feedback on the stepfile difficulty of a given stepfile. Each time Player $A_i$ for all $1 \leq i \leq n$ makes an assessment on the difficulty of the stepfile, they rely on both unique characteristics of the stepfile and their individual skill sets
to justify this claim. Under the assumption that stepfile difficulty is additive, this can captured in the following equation:

$$\text{Diff}_{\text{obs}}^{(A_i)} = \text{Diff}_{\text{obj}}^{(A_i)} + \text{Diff}_{\text{sub}}^{(A_i)}.$$

Because each player $A_1, A_2, \cdots, A_n$ are exposed to the same step layout in a given stepfile, their unbiased assessments of difficulty are mutually shared. This can be mathematically expressed as follows:

$$\text{Diff}_{\text{obj}} \coloneq \text{Diff}_{\text{obj}}^{(A_1)} = \text{Diff}_{\text{obj}}^{(A_2)} = \cdots = \text{Diff}_{\text{obj}}^{(A_n)}.$$

Define $A = (A_1, \cdots, A_n)$ to represent a community of $n$ players. When we aggregate all $n$ player's difficulty assessments, we can determine the observed difficulty rating of a stepfile subject to community bias:

$$\text{Diff}_{\text{obs}}^{(A)} = \text{Diff}_{\text{obj}} + \text{Diff}_{\text{sub}}^{(A)}.$$

In theory, by obtaining feedback from the entire population regarding the difficulty of a stepfile, with an unlimited sample (i.e. $n \rightarrow \infty$), it may be possible to establish an accurate measurement of difficulty that considers human limitations and reduces the influence of player subjectivity. Therefore, for some constant unknown value $\text{Diff}_{\text{human}}$, we have the following:

$$\lim_{n \rightarrow \infty} \text{Diff}_{\text{obs}}^{(A)} = \text{Diff}_{\text{true}}\mspace{50mu} \text{and} \mspace{50mu} \lim_{n \rightarrow \infty} \text{Diff}_{\text{sub}}^{(A)} = \text{Diff}_{\text{human}}.$$

Using the above asymptotics, we can conclude that
$$\text{Diff}_{\text{true}} = \text{Diff}_{\text{obj}} + \text{Diff}_{\text{human}}.$$
\end{proof}

Note that the current difficulty ratings in game are notated as $\text{Diff}_{\text{obs}}^{(A)}$ in the previous proof. Based on this, we propose the following corollary:

\begin{corollary}
The deviation between $\text{Diff}_{\text{true}}$ and $\text{Diff}_{\text{obs}}^{(A)}$ is largely dependent on subjective features of the stepfile defined by the vocal community. In other words,

$$|\text{Diff}_{\text{obs}}^{(A)} - \text{Diff}_{\text{true}}| = |\text{Diff}_{\text{sub}}^{(A)} - \text{Diff}_{\text{human}}|$$
\end{corollary}

In order to ensure the credibility of the given target values, it is crucial to minimize the deviation between $\text{Diff}_{\text{obs}}^{(A)}$ and $\text{Diff}_{\text{true}}$. From the above corollary, this can be accomplished by utilizing community feedback to estimate $\text{Diff}_{\text{human}}$, the role which difficulty consultants play when receiving new opinions from community members. This requires us to gather more feedback from the community, which is highly susceptible to self-selection bias and group attribution error.

\vspace{2mm}

Instead of attempting to correct data labeling errors, we will use the ratings as targets in our supervised learning problem to estimate the true difficulty $\text{Diff}_{\text{true}}$ by maximizing the explained variation of the objective stepfile features. Doing so enables us to rely less on our inability to control identified biases in our analysis, while creating future opportunities to improve the definition of true difficulty through through continued research on player telemetry and improved feature engineering from available stepfile data.
\vspace{2mm}

Let's assume that there is some relationship between $Y \coloneq  \text{Diff}_{\text{true}}$ and features $X_{\text{obj}} = (X_{\text{obj}}^1, \cdots, X_{\text{obj}}^n)$ which can be written in the very general form:

$$Y = f(X_{\text{obj}}) + \epsilon$$

where $f$ is some fixed and unknown function of $X_{\text{obj}}^1, \cdots, X_{\text{obj}}^n$ and $\epsilon$ is a random error term, independent of $X_{\text{obj}}$ and has mean zero. Given our previously stated goal, it is important to note that the error term is specifically formulated to account for the explained variance of stepfile difficulty that can be attributed to inherent human limitations.

\vspace{2mm}

Let $\hat{f}$ be an estimate of $f$ that yields prediction $\hat{Y} = \hat{f}(X_{\text{obj}})$. Assume for a moment that both $\hat{f}$ and $X_{\text{obj}}$ are fixed. Then,
\begin{align*}
    E(Y - \hat{Y})^2 & = E(f(X_{\text{obj}}) + \epsilon - \hat{f}(X_{\text{obj}}))^2 \\
    & = [f(X_{\text{obj}}) - \hat{f}(X_{\text{obj}})]^2 + \text{Var}(\epsilon)
\end{align*}

The main objective of our research is to accurately calculate the function $f$, while also minimizing the reducible error $[f(X_{\text{obj}}) - \hat{f}(X_{\text{obj}})]^2$. It is important to acknowledge that the features selected in this study will comply with inferential properties related to stepfile difficulty, as further explained in a subsequent section. 
\vspace{2mm}






\textbf{Plan:}
\begin{itemize}
	% \item Untangle objective definitions of difficulty from its inherent subjectivity. Identify that the goal of creating this machine learning model is to maximize the objective representation of difficulty in the overall model.
	      	      
	\item Clarify the definition of difficulty by introducing judge window sizes. Also dissociate processing difficulty from execution difficulty and focus paper on execution. Add visuals:
	      \begin{itemize}
	      	\item Difficulty to FC: 235 ms
	      	\item Difficulty to Clean FC: 201 ms
	      	\item Difficulty to Maximize Score: 153.9 ms
	      	\item Difficulty to Maximize Score under MS based scoring: 117.5 ms
	      	\item Difficulty to AAA: 100 ms
	      	\item Difficulty to AAAA: 34 ms
	      \end{itemize}
	\item Address concerns about inaccurate difficulty measurements current in game and the non-existing idea of "true" difficulty using crowdsourcing papers \href{https://crowdsourcing-class.org/readings/downloads/quality-control/Quality-Management-on-Amazon-Mechanical-Turk.pdf#page4}{here} and \href{https://crowdsourcing-class.org/readings/downloads/ml/EM.pdf}{here}.
	      	      
	      \item{Introduce Contested Difficulty Sheet and define train-test split using KDE estimations of proposed difficulties}
	      	      
	      \item{Introduce objective features VerticalDensity and HorizontalDensity using pen-tapping as an example. Also mention chart augmentation considerations:}
	      \begin{itemize}
	      	\item Mirror: Indirectly accounted in VerticalDensity's definition
	      	\item Offset: Indirectly accounted in preprocessing of FFR's API response.
	      	\item Colors: Categorized under "processing difficulty"; users can alter this.
	      	\item Scroll Speed Mod: Categorized under "processing difficulty"; users can alter this.
	      \end{itemize}
	      	      
	\item Dissociate local features from global features. Song length is a global feature, subsequence extraction is a local feature. Mention how ensembling works with the consideration of "difficulty spikes" in many charts.
	      	      
	\item Introduce model architecture and implementation. Consider regressing over the target variable transformations of difficulty due to domain knowledge of difficulty.
	      	      
	\item Run experiments with proposed model, share results, and identify method to measure the quality of the predictions.
	      	      
	\item Extend proposed model to account for uprated and downrated stepfiles, run experiments and compare model performance with already tracked uprated scores.
	      	          
\end{itemize}

\vspace{2mm}


% In addition to issues mentioned earlier in this report, the evolution of FFR's stepfile difficulty measurement demonstrates the varying perspectives on what constitutes a challenging stepfile. A notable factor contributing to this lack of a consensus is the absence of standardized criteria for determining difficulty. This section aims to establish such standards in order to establish a more reliable and trusted model for evaluating stepfile difficulty, by examining the Contested Difficulties spreadsheet and identifying the difficulty values that hold the most consensus within the FFR community.

\subsection{Contested Difficulty Spreadsheet Analysis}

The Contested Difficulty Spreadsheet, managed by \texttt{Zlyice}, is a collaborative Google Sheets document utilized for tracking proposed changes to stepfile difficulty measurements based on input from users. It is presumed that the individual suggesting a difficulty measurement possesses a degree of knowledge regarding the factors that warrant such a rating for the stepfile. While it must be acknowledged that all proposed measurements are inherently vulnerable to volunteer bias, the process of crowd-sourcing these partial inputs serves to reduce the impact of individual bias on any given stepfile. This approach aims to align the definition of difficulty with the perspectives of the vocal minority within the wider community.

\vspace{2mm}


In order to align our model scoring with the prevailing consensus on the quantification of stepfile difficulty within the community, we conduct a thorough analysis of the proposed difficulty levels over the period spanning from 2022 to the present date. This analysis serves as the basis for our development of a train-test split strategy, in which the distribution of proposed difficulties is utilized to allocate our data into appropriately representative training, validation, and test sets.